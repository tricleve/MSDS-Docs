{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6baab44a",
   "metadata": {},
   "source": [
    "# DS7331 Project 3\n",
    "#### Group 2: Hollie Gardner, Cleveland Johnson, Shelby Provost\n",
    "[Dataset Source](https://archive-beta.ics.uci.edu/ml/datasets/census+income)<br/>\n",
    "[Github Repo](https://github.com/ShelbyP27/DS7331-Project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "329f550a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shelb\\anaconda3\\envs\\DS7331\\lib\\site-packages\\IPython\\extensions\\rmagic.py:11: UserWarning: The rmagic extension in IPython has moved to `rpy2.ipython`, please see `rpy2` documentation.\n",
      "  warnings.warn(\"The rmagic extension in IPython has moved to \"\n",
      "C:\\Users\\shelb\\anaconda3\\envs\\DS7331\\lib\\site-packages\\rpy2\\robjects\\pandas2ri.py:17: FutureWarning: pandas.core.index is deprecated and will be removed in a future version.  The public classes are available in the top-level namespace.\n",
      "  from pandas.core.index import Index as PandasIndex\n"
     ]
    },
    {
     "ename": "RRuntimeError",
     "evalue": "Error in loadNamespace(name) : there is no package called 'arules'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRRuntimeError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23852/4014829200.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;31m#     install.package(arulesViz)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'C:/Users/johnc45/R/R-Library'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m \u001b[0marules\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'arules'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlib_loc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# same as importing in R with the \"library\" command\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[0marules_viz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimportr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'arulesViz'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlib_loc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlib\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# visualize the different rules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\DS7331\\lib\\site-packages\\rpy2\\robjects\\packages.py\u001b[0m in \u001b[0;36mimportr\u001b[1;34m(name, lib_loc, robject_translations, signature_translation, suppress_messages, on_conflict, symbol_r2python, symbol_check_after, data)\u001b[0m\n\u001b[0;32m    451\u001b[0m     if _package_has_namespace(rname, \n\u001b[0;32m    452\u001b[0m                               _system_file(package = rname)):\n\u001b[1;32m--> 453\u001b[1;33m         \u001b[0menv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_namespace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    454\u001b[0m         \u001b[0mversion\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_namespace_version\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    455\u001b[0m         \u001b[0mexported_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_get_namespace_exports\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRRuntimeError\u001b[0m: Error in loadNamespace(name) : there is no package called 'arules'\n"
     ]
    }
   ],
   "execution_count": 1,
   "id": "329f550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sklearn as sk\n",
    "#import lazypredict\n",
    "\n",
    "# data visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# data preprocessing \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "#pca and gridsearch\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#prediction models\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#exceptional work (working on this)\n",
    "import math \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea39c781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from michael hahsler's github -- https://mhahsler.github.io/arules/docs/python/arules_python.html\n",
    "from rpy2.robjects import pandas2ri\n",
    "pandas2ri.activate()\n",
    "\n",
    "import rpy2.robjects as ro\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "arules = importr(\"arules\")\n",
    "\n",
    "# some helper functions\n",
    "def arules_as_matrix(x, what = \"items\"):\n",
    "    return ro.r('function(x) as(' + what + '(x), \"matrix\")')(x)\n",
    "\n",
    "def arules_as_dict(x, what = \"items\"):\n",
    "    l = ro.r('function(x) as(' + what + '(x), \"list\")')(x)\n",
    "    l.names = [*range(0, len(l))]\n",
    "    return dict(zip(l.names, map(list,list(l))))\n",
    "\n",
    "def arules_quality(x):\n",
    "    return x.slots[\"quality\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "894f6f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/IPython/extensions/rmagic.py:11: UserWarning: The rmagic extension in IPython has moved to `rpy2.ipython`, please see `rpy2` documentation.\n",
      "  warnings.warn(\"The rmagic extension in IPython has moved to \"\n"
     ]
    }
   ],
   "source": [
    "#association rules\n",
    "import time\n",
    "%matplotlib inline\n",
    "from rpy2.robjects.packages import importr\n",
    "from rpy2 import robjects as robj\n",
    "\n",
    "%load_ext rmagic\n",
    "%load_ext rpy2.ipython \n",
    "# this enables the %R extension to iPython (does not work outside of the iPython shell)\n",
    "\n",
    "# these packages will need to be installed\n",
    "# open R and run \n",
    "#     install.package(arules)\n",
    "#     install.package(arulesViz)\n",
    "#lib='C:/Users/johnc45/R/R-Library'\n",
    "#arules = importr('arules', lib_loc=lib) # same as importing in R with the \"library\" command\n",
    "#arules_viz = importr('arulesViz', lib_loc=lib) # visualize the different rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8481c964",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfa6180",
   "metadata": {},
   "source": [
    "### Loading and Prepping Data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "execution_count": 4,
   "id": "2f225805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the census dataset using pandas\n",
    "# Reading the CSV file after converting file to csv and removing superfluous spaces via Excel.\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/ShelbyP27/DS7331-Project/main/adult-data.csv')\n",
    "\n",
    "# Getting a first look at the dataset\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "execution_count": 5,
   "id": "1faf4f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cleaning up data set\n",
    "df = df.replace(to_replace='?',value=np.nan) # replace '?' with NaN (not a number)\n",
    "df.dropna(inplace=True) # Removing na values\n",
    "df.duplicated(subset=None, keep='first') #Remove duplicates\n",
    "df['income'] = df['income'].map({'<=50K': 0, '>50K': 1}).astype(int) #One-hot respone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "execution_count": 6,
   "id": "ff4a638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# One-hot encoding the Categorical variables\n",
    "if 'sex' in df:\n",
    "    df['IsMale'] = df.sex == 'Male'\n",
    "    df.IsMale = df.IsMale.astype(np.int64)\n",
    "    del df['sex']\n",
    "    \n",
    "if 'marital-status' in df:\n",
    "    tmp_df = pd.get_dummies(df['marital-status'], prefix = 'Marital')\n",
    "    df = pd.concat((df, tmp_df), axis =1)\n",
    "    del df['marital-status']\n",
    "    \n",
    "if'relationship' in df:\n",
    "    tmp_df = pd.get_dummies(df['relationship'], prefix = 'Rel')\n",
    "    df = pd.concat((df, tmp_df), axis =1)\n",
    "    del df['relationship']\n",
    "\n",
    "if 'race' in df:\n",
    "    tmp_df = pd.get_dummies(df['race'], prefix = 'Race')\n",
    "    df = pd.concat((df, tmp_df), axis =1)\n",
    "    del df['race']\n",
    "\n",
    "if 'workclass' in df:\n",
    "    tmp_df = pd.get_dummies(df['workclass'], prefix = 'Work')\n",
    "    df = pd.concat((df, tmp_df), axis =1)\n",
    "    del df['workclass']\n",
    "\n",
    "if 'occupation' in df:\n",
    "    tmp_df = pd.get_dummies(df['occupation'], prefix = 'Occupation')\n",
    "    df = pd.concat((df, tmp_df), axis =1)\n",
    "    del df['occupation']\n",
    "\n",
    "if 'education' in df:\n",
    "    tmp_df = pd.get_dummies(df['education'], prefix = 'Education')\n",
    "    df = pd.concat((df, tmp_df), axis =1)\n",
    "    del df['education']\n",
    "\n",
    "    \n",
    "#Replace Native Country with Immigrant atribute\n",
    "if 'native-country' in df:\n",
    "    df['immigrant'] = np.where(df['native-country']!= 'United-States', 1, 0)\n",
    "    del df['native-country']\n",
    "\n",
    "df2 = df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "execution_count": 7,
   "id": "31eda3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from the response\n",
    "if 'income' in df:\n",
    "    y = df['income'].values\n",
    "    del df['income']\n",
    "    X = df.values\n",
    "\n",
    "# Train / Test split with scaled_X\n",
    "scaled_X = StandardScaler().fit_transform(X)\n",
    "x_train, x_test, y_train, y_test = train_test_split(scaled_X, y, test_size = .2, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "execution_count": 8,
   "id": "109b08c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xnew = df[['age','hours-per-week', 'capital-loss','education-num','Marital_Never-married','Marital_Married-civ-spouse','Work_Private', 'Rel_Husband','capital-gain','Rel_Not-in-family']].values\n",
    "\n",
    "newscaled_X = StandardScaler().fit_transform(Xnew)\n",
    "x_train, x_test, y_train, y_test = train_test_split(newscaled_X, y, test_size = .2, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbba164d",
   "metadata": {},
   "source": [
    "## Business Understanding\n",
    "*Describe the purpose of the data set you selected (i.e., why was this data collected in the first place?). How will you measure the effectiveness of a good algorithm? Why does your chosen validation method make sense for this specific dataset and the stakeholders needs?*\n",
    "\n",
    "<br>Census data is regularly collected by the US Census Bureau in order for the government to understand the characteristics of US population. Data collected from every household in the US through census surveys typically includes details on a number of items such as number of people in the household, income, profession, race/ethnity, languages spoken, sex, age, etc. This dataset was extracted from the 1994 Census database by Barry Becker for the purpose of predicting if an individual's income is greater than $50k.\n",
    "\n",
    "<br>The outcome of this data set is to assess if an indivdual has an income greater than &dollar;50k. Since the data set was given with the income listed as greater than 50k or less than 50k, the definition of the outcome is straight foward. We would measure the outcome by assessing the predictor variables for significance in predicting an individuals income. The statistically significant variables should have a minimal error in predictability.\n",
    "\n",
    "<br>The effectiveness of a good prediction algorithm would be measured through the error rates in the algorithm's ability to classify and learn from the training data and predict the individual income categories in the test set based off the attributes provided.\n",
    "\n",
    "-- Why does your chosen validation method make sense for this specific dataset and the stakeholders needs? --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b15d4d5",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "*Part One: Describe the meaning and type of data (scale, values, etc.) for each attribute in the data file. Verify data quality: Are there missing values? Duplicate data? Outliers? Are those mistakes? How do you deal with these problems?*\n",
    "\n",
    "This dataset contains 15 attributes for 32,560 instances in the training set. The test set includes an additional 16,281 instances. In addition to the code below, details were obtained from the adult.names file provided by dataset donors from the above linked UCI website.\n",
    "\n",
    "The attribute descriptions are as follows:\n",
    "* **age** (integer, range 17-90 ): This is the age of the individual. \n",
    "* **workclass** (categorical, 9 levels): Work class is the sector or category of the individual's occupation\n",
    "* **fnlwgt** (integer, range 13769-1484705): The \"final weight\" attribute was calculated from the Current Population Survey and \"refers to population totals derived from CPS by creating \"weighted tallies\" of any specified socio-economic characteristics of the population.\" (See full description of this attribute from Kohavi and Becker below)\n",
    "* **education** (categorical, 16 levels): Highest level of education the individual has obtained\n",
    "* **education-num** (integer, range 1-16): ___\n",
    "* **marital-status** (categorical, 7 levels): The individual's marital status\n",
    "* **occupation** (categorical, 15 levels): The type of work an individual performs\n",
    "* **relationship** (categorical, 6 levels): ____\n",
    "* **race** (categorical, 5 levels): Individual's racial group\n",
    "* **sex** (categorical, 2 levels): Male or Female\n",
    "* **capital-gain** (integer, range 0-99999): _____\n",
    "* **capital-loss** (integer, range 0-4356): _____\n",
    "* **hours-per-week** (integer, range 1-99): Number of hours per week individual works in occupation\n",
    "* **native-country** (categorical, 42 levels): The individual's native country\n",
    "* **income** (categorical, 2 levels): This has been converted from the raw data to be less than or equal to $50K or greater than $50K\n",
    "<br>\n",
    "<br>**Kohavi and Becker Description of Final Weight from adult.names file:**\n",
    "<br>\n",
    "<br>| The weights on the CPS files are controlled to independent estimates of the\n",
    "<br>| civilian noninstitutional population of the US.  These are prepared monthly\n",
    "<br>| for us by Population Division here at the Census Bureau.  We use 3 sets of\n",
    "<br>| controls.\n",
    "<br>|  These are:\n",
    "<br>|          1.  A single cell estimate of the population 16+ for each state.\n",
    "<br>|          2.  Controls for Hispanic Origin by age and sex.\n",
    "<br>|          3.  Controls by Race, age and sex.\n",
    "<br>|\n",
    "<br>| We use all three sets of controls in our weighting program and \"rake\" through\n",
    "<br>| them 6 times so that by the end we come back to all the controls we used.\n",
    "<br>|\n",
    "<br>| The term estimate refers to population totals derived from CPS by creating\n",
    "<br>| \"weighted tallies\" of any specified socio-economic characteristics of the\n",
    "<br>| population.\n",
    "<br>|\n",
    "<br>| People with similar demographic characteristics should have\n",
    "<br>| similar weights.  There is one important caveat to remember\n",
    "<br>| about this statement.  That is that since the CPS sample is\n",
    "<br>| actually a collection of 51 state samples, each with its own\n",
    "<br>| probability of selection, the statement only applies within\n",
    "<br>| state.\n",
    "\n",
    "<br><br>\n",
    "    According to the Kohavi and Becker, the following changes have already been made to the raw data:\n",
    "* Discretized gross income into two ranges with threshold 50,000.\n",
    "* Convert U.S. to US to avoid periods.\n",
    "* Convert Unknown to \"?\"\n",
    "* Run MLC++ GenCVFiles to generate data,test.\n",
    "<br>\n",
    "\n",
    "Therefore, we will begin by looking for the '?' values in the dataset by counting the number in each column. Then, we will replace as null using numpy. We decided to handle missing values in the following three variables which possessed missing values as below: \n",
    "\n",
    "* Workclass: 1836 rows\n",
    "* Occupation: 1843 rows\n",
    "* Native-Country: 583 rows\n",
    "\n",
    "There is not a way to impute values for these categorical variables and leaving as unknown variables does not add value to our analysis. Therefore, these rows will be removed leaving us with 30,162 complete rows in this dataframe.\n",
    "\n",
    "As for duplicates, Kohavi and Becker described 6 duplicate rows. After reviewing our current dataframe, there do not appear to be any instances of duplicate data.\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaeb422b",
   "metadata": {},
   "source": [
    "## Modeling and Evaluation\n",
    "*Different tasks will require different evaluation methods. Be as thorough as possible when analyzing the data you have chosen and use visualizations of the results to explain the performance and expected outcomes whenever possible. Guide the reader through your analysis with plenty of discussion of the results.*\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1424b790",
   "metadata": {},
   "source": [
    "### Option A: Cluster Analysis\n",
    "\n",
    " - Perform cluster analysis using several clustering methods\n",
    " - How did you determine a suitable number of clusters for each method?\n",
    " - Use internal and/or external validation measures to describe and compare the clusterings and the clusters (some visual methods would be good).\n",
    " - Describe your results. What findings are the most interesting and why? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a771cda",
   "metadata": {},
   "source": [
    "### Option B: Association Rule Mining\n",
    " - Create frequent itemsets and association rules.\n",
    " - Use tables/visualization to discuss the found results.\n",
    " - Use several measure for evaluating how interesting different rules are.\n",
    " - Describe your results. What findings are the most compelling and why? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5a88de6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apriori\n",
      "\n",
      "Parameter specification:\n",
      " confidence minval smax arem  aval originalSupport maxtime support minlen\n",
      "        0.8    0.1    1 none FALSE            TRUE       5    0.05      2\n",
      " maxlen target  ext\n",
      "     10  rules TRUE\n",
      "\n",
      "Algorithmic control:\n",
      " filter tree heap memopt load sort verbose\n",
      "    0.1 TRUE TRUE  FALSE TRUE    2    TRUE\n",
      "\n",
      "Absolute minimum support count: 1508 \n",
      "\n",
      "set item appearances ...[0 item(s)] done [0.00s].\n",
      "set transactions ...[70 item(s), 30162 transaction(s)] done [0.20s].\n",
      "sorting and recoding items ... [70 item(s)] done [0.02s].\n",
      "creating transaction tree ... done [0.04s].\n",
      "checking subsets of size 1 2 3 4 5 done [7.76s].\n",
      "writing ... [51982494 rule(s)] done [27.52s].\n",
      "creating S4 object  ... done [26.18s].\n"
     ]
    }
   ],
   "source": [
    "%R -i df rules <- apriori(df,parameter = list(minlen=2, supp=0.05, conf=0.8))\n",
    "%R rules.sorted <- sort(rules, by='lift')\n",
    "%R plot(rules.sorted, method='grouped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6690cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%R -o df_from_R df_from_R<-df\n",
    "\n",
    "# now we have the exact same dataset as the one from R\n",
    "# but it is now a pandas dataframe\n",
    "print(df_from_R.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb73cd7",
   "metadata": {},
   "source": [
    "### Option C: Collaborative Filtering\n",
    " - Create user-item matrices or item-item matrices using collaborative filtering\n",
    " - Determine performance of the recommendations using different performance measures and explain what each measure\n",
    " - Use tables/visualization to discuss the found results. Explain each visualization in detail.\n",
    " - Describe your results. What findings are the most compelling and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6315d02f",
   "metadata": {},
   "source": [
    "## Deployment\n",
    " - Be critical of your performance and tell the reader how you current model might be usable by other parties. Did you achieve your goals? If not, can you reign in the utility of your modeling?\n",
    " - How useful is your model for interested parties (i.e., the companies or organizations that might want to use it)?\n",
    " - How would your deploy your model for interested parties?\n",
    " - What other data should be collected?\n",
    " - How often would the model need to be updated, etc.? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f79b8f",
   "metadata": {},
   "source": [
    "## Exceptional Work\n",
    " - You have free reign to provide additional analyses or combine analyses "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
